---
title: "General structure of a simulation study [Assignment]"
subtitle: "Comparing the false-positive rate between Student's and Welches' independent t-tests"
author: "[Tina P]"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# Check your learning

## - What are the five essential components of a simulation study?
    *1.	Generate pseudo-random data set with known properties*
    *2.	Analyse data with a statistical method*
    *3.	Repeat 1 & 2 many times (‘iterations’)*
    *4.	Collect and aggregate results across iterations*
    *5.	Make it an experiment: Systematically vary parameters in Step 1 (between factor) and/or compare different ways to do Step 2 (within factor)*

## - Can you explain what `tidyr::expand_grid()` does (in the context of a simulation)? 
*generates all possible combinations of values from the specified vectors. --> to explore different combinations of these parameters*
*Returns a tibble, not a data frame.*
        *Each row in the resulting data frame represents a unique combination of parameters, and you can use this grid to conduct simulations for each combination.*
        *This function is particularly handy for setting up parameter grids in simulation studies or optimization experiments where you want to explore the effect of different parameter values on the outcome of your simulations.*
```{r}
library(tidyr)
#?expand_grid


#Exampel
# Define parameter values
flips <- c(10, 20, 30)
prob_heads <- c(0.3, 0.5, 0.7)

# Create a data frame with all combinations
explain_simulation_grid <- expand_grid(flips = flips, prob_heads = prob_heads)



```
## - Can you explain what `purrr::pmap()` does (in the context of the simulation workflow above)? 

*short: with this function you can iterate over multiple arguments simultaneously*
*arguments should be a list of vectors!*

*pmap = "parallel map." It is designed to apply a given function to multiple sets of parameters in parallel.*
*1. Define a Function; 2. Provide Parameter Values (exp. with expand_grid) --> pmap(parameter_grid, function)*
*3.Parallel Mapping:*
      *vpmap() then applies the specified function to each set of parameters in parallel.*
      *It iterates over the rows of the data frame and applies the function with the corresponding parameter values.*
*The result is a list containing the outputs of applying the function to each set of parameters.*

```{r}

#Exampel

library(purrr)
#?pmap

# Define a simulation function
simulate_coin_flip <- function(flips, prob_heads) {
  # Your simulation logic here, e.g., using rbinom for coin flips
  explain_simulated_results <- rbinom(flips, 1, prob_heads)
  # Return some summary or result of the simulation
  return(list(flips = flips, prob_heads = prob_heads, result = sum(explain_simulated_results)))
}

# Create a data frame with parameter combinations
explain_parameter_grid <- expand_grid(flips = c(10, 20, 30), prob_heads = c(0.3, 0.5, 0.7))

# Apply simulation function to each set of parameters in parallel
explain_simulation_results <- pmap(explain_parameter_grid, simulate_coin_flip)



```

## - Broadly speaking, what does changing the number of iterations used in a simulation do to the results of that simulation, and why? What analogy can be drawn with experiments with real participants?
    *The standard error remains large if only a few iterations are made (exp. 100, 1000 would be better) and as in experiments with humans: sample size matters.*
    *In real experiments, increasing the number of participants often leads to more reliable and generalizable results.*
    *Larger sample sizes improve statistical power, reduce sampling variability, and increase the precision of estimates.*


# Apply your learning

The code in the below chunk is a copy of the final working simulation from "1_general_structure_of_a_simulation.Rmd". Modify the code appropriately to create a new simulation that demonstrates the following: 

*The Student's independent t-test assumes both equal variances (SDs) and equal sample sizes between groups. When this assumption is violated, it should suffer from an inflated false-positive rate (i.e., more than 5% of p values will be significant when alpha = .05). The Welches' t-test does not make this assumption: when the two groups have both different variances (SDs) and different Ns, the false-positive rate should remain at the alpha level. Demonstrate that this is the case via simulation.*

*To do this, you should modify the simulation in the following ways.

[x] Use the parameters listed in the below code chunk 'as-is': I've already modified it to contain the settings I want you to simulate for this question. 
[ ] Once you have a working simulation, you can also simulate for other parameters if you want to understand the conditions that     determine when you find differences between the tests.

[x] Remove the Cohen's d calculation as we don't need it here.

[x] Compare the performance of the Student's independent t-test with a Welches' independent t-test. To do this, write a second data analysis function by copying and then modifying the existing one so that it saves the p value for a Welches' test instead.

[x] Both data analysis functions should be applied to same dataset, i.e., one generated dataset that is analysed in two different way. The p values of the two different tests should be saved as separate columns.

[x] To summarize the results of your new simulation, calculate the false positive rate (proportion of cases where p < .05) for each experimental condition.

[x] Try to also create histogram(s) that plot the distribution of p values for each of the two tests.

Remember that in both the previous simulation and this one you're studying the proportion of tests that produce a significant result. However, in the previous simulation the population effect size was non-zero, so the proportion of significant effects represents the tests statistical power (the true-positive rate). However, in this stimulation the population effect size is zero, so the proportion of significant effects represents the test's false-positive rate.

When modifying the code:

- Don't forget you can decrease the number of iterations during development and then increase it when you're ready to run the study for real. 
- Make modifications, do bug checking, and even make git commits incrementally. Think about not only getting the final simulation working, but the process of how you got there through making changes, checking that those changes produce the expected outcomes, and then moving on to the next task.
- If you're stuck, please post about it on Slack. I need feedback to determine whether this exercise is too easy or too hard for the group.

## Dependencies
```{r}

# # remove all objects from environment ----
rm(list = ls())


# dependencies ----

library(tidyr)
library(dplyr)
library(forcats)
library(readr)
library(purrr) 
library(ggplot2)
library(effsize)


# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(42)
```


## new Simulation
### generate data
```{r}

# define data generating function ----
generate_data <- function(n_control,
                          n_intervention,
                          mean_control,
                          mean_intervention,
                          sd_control,
                          sd_intervention) {
  
  data <- 
    bind_rows(
      tibble(condition = "control",
             score = rnorm(n = n_control, mean = mean_control, sd = sd_control)),
      tibble(condition = "intervention",
             score = rnorm(n = n_intervention, mean = mean_intervention, sd = sd_intervention))
    ) |>
    # control's factor levels must be ordered so that intervention is the first level and control is the second
    # this ensures that positive cohen's d values refer to intervention > control and not the other way around.
    mutate(condition = fct_relevel(condition, "intervention", "control"))
  
  return(data)
}
```


### two ata analysis functions
- Compare the performance of the Student's independent t-test with a Welches' independent t-test. To do this, write a second data analysis function by copying and then modifying the existing one so that it saves the p value for a Welches' test instead.

copied the original analysis
Welches' independent t-test --> set var.equal FALSE --> _w
T-Test --> _t

```{r}

## T Test ~ added a _t to differ between the t-test and the welch test
# define data analysis function ----
analyse_data_t <- function(data) {
  
  res_t_test <- t.test(formula = score ~ condition, 
                       data = data,
                       var.equal = TRUE,
                       alternative = "two.sided")
  
  
  res <- tibble(p = res_t_test$p.value)
  
  return(res)
}



# Welches' independent t-test --> set var.equal FALSE

# define data analysis function ----
analyse_data_w <- function(data) {
  
  res_t_test <- t.test(formula = score ~ condition, 
                       data = data,
                       var.equal = FALSE, # WELCH TEST
                       alternative = "two.sided")
  
  
  res <- tibble(p = res_t_test$p.value)
  
  return(res)
}

```


### experiment parameters grid

100 --> 1000
```{r}
# define experiment parameters ----
# note: these are the correct ones I want you to simulate. 
experiment_parameters_grid <- expand_grid(
  n_control = 50, # notice that the two sample sizes are different
  n_intervention = 25,
  mean_control = 0, # notice that the two means are both 0: the population difference in means is null.
  mean_intervention = 0,
  sd_control = 0.66, # notice that the two sample variances (via SDs) are different
  sd_intervention = 1.33,
  iteration = 1:1000 # the final simulation needs lots of p values for a good plot
)

```

### Simulation with the p-value columns
- Both data analysis functions should be applied to same dataset, i.e., one generated dataset that is analysed in two different way. The p values of the two different tests should be saved as separate columns.

added another column for the p-values from the Welch Test
```{r}
# run simulation ----
simulation <- 
  # using the experiment parameters
  experiment_parameters_grid |>
  
  # generate data using the data generating function and the parameters relevant to data generation
  mutate(generated_data = pmap(list(n_control,
                                    n_intervention,
                                    mean_control,
                                    mean_intervention,
                                    sd_control,
                                    sd_intervention),
                               generate_data)) |>
  
  # apply the analysis function to the generated data using the parameters relevant to analysis
  ## added another column for the p-values from the Welch Test
  mutate(analysis_results_T = pmap(list(generated_data),
                                 analyse_data_t),
         analysis_results_W = pmap(list(generated_data),
                                 analyse_data_w))
  
```

### Summarize Simulation
- To summarize the results of your new simulation, calculate the false positive rate (proportion of cases where p < .05) for each experimental condition.

unnested both analysis, than renamed the columns
I think? I calculated the false positive rate?
```{r}
# summarise simulation results over the iterations ----
## estimate power  
## ie what proportion of p values are significant (< .05)


simulation_summary <- simulation |>
  unnest(c(analysis_results_T, analysis_results_W), names_sep = "_") |>
  rename(p_T = analysis_results_T_p,
         p_W = analysis_results_W_p)|>    
  mutate(n_control = as.factor(n_control),
         n_intervention = as.factor(n_intervention))|>
  group_by(n_control, n_intervention)  |>
  summarize(false_pos_T = mean(p_T < 0.05),
            false_pos_W = mean(p_W < 0.05),
            .groups = "drop")


```

### Histograms
- Try to also create histogram(s) that plot the distribution of p values for each of the two tests.
```{r}

#preparing the dataset for the plots
simulation_plots <- simulation |>
  unnest(c(analysis_results_T, analysis_results_W), names_sep = "_") |>
  rename(p_T = analysis_results_T_p,
         p_W = analysis_results_W_p)|>    
  mutate(n_control = as.factor(n_control),
         n_intervention = as.factor(n_intervention))|>
  group_by(n_control, n_intervention)  



# plot for the T Test
ggplot(simulation_plots, aes(p_T)) +
  geom_histogram(fill = "darkslategray2", color = "darkslategray4") +
  ggtitle("distribution of p values for the T-Test") +
  ylab("frequencies")+
  ylim(0, 65) +
  xlab("p value")

# plot for the W Test
ggplot(simulation_plots, aes(p_W)) +
  geom_histogram(fill = "gold", color = "gold4") +
  ggtitle("distribution of p values for the T-Test") +
  ylab("frequencies")+
   ylim(0, 65) +
  xlab("p value")


```

# Session info

```{r}

sessionInfo()

```


